{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avangelizer/internship/blob/master/main_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPSj--ECif1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3aaf632e-cf47-46aa-fc54-fb165eb1e96b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pebileftjt3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip \"/gdrive/My Drive/syringodium_20_slices_enhanced.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFR_pbnDsB9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pretrainedmodels\n",
        "!pip install torchvision tensorboardX\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWyaQSDJM6QB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee5a51ed-50d4-4faf-d59d-481e87bb7367"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from typing import Dict, Callable, List\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "from callbacks import MixUpCallback,LearningRateSchedulerCallback\n",
        "from schedulers import TriangularLR\n",
        "from weight_decay import WeightDecayOptimizerWrapper, setup_differential_learning_rates\n",
        "from metrics import FBeta\n",
        "from sklearn.metrics import fbeta_score, log_loss\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, cuda\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "from Bot import Bot\n",
        "from models import get_seresnet_model, get_densenet_model, get_seresnet_partial_model\n",
        "from transform_dataset import TrainDataset, TestDataset, get_transform_test, get_transform_train\n",
        "from loss import FocalLoss\n",
        "\n",
        "CACHE_DIR = Path('/content/cache/')\n",
        "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "MODEL_DIR = Path('/content/models/')\n",
        "MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
        "DATA_ROOT=Path('/content/content/output_syr_slices')\n",
        "\n",
        "def make_loader(args, ds_class, root, df: pd.DataFrame, image_transform, drop_last=False, shuffle=False) -> DataLoader:\n",
        "    return DataLoader(\n",
        "        ds_class(root, df, image_transform),\n",
        "        shuffle=shuffle,\n",
        "        batch_size=args.batch_size,\n",
        "        drop_last=drop_last\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class ImageClassificationBot(Bot):\n",
        "    checkpoint_dir: Path = CACHE_DIR / \"model_cache/\"\n",
        "    log_dir: Path = MODEL_DIR / \"logs/\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super().__post_init__()\n",
        "        self.loss_format = \"%.6f\"\n",
        "        self.metrics = (FBeta(step=0.05, beta=2, average=\"samples\"),)\n",
        "        self.monitor_metric = \"fbeta\"\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "def train_stage_one(args, model, train_loader, valid_loader, criterion):\n",
        "    optimizer = WeightDecayOptimizerWrapper(\n",
        "        torch.optim.Adam(model.parameters(), lr=2e-3),\n",
        "        0.1\n",
        "    )\n",
        "    freeze_layers(model, [True, True, False])\n",
        "\n",
        "    # stage 1\n",
        "    n_steps = len(train_loader) // 2\n",
        "    bot = ImageClassificationBot(\n",
        "        model=model, train_loader=train_loader,\n",
        "        val_loader=valid_loader, clip_grad=10.,\n",
        "        optimizer=optimizer, echo=True,\n",
        "        criterion=criterion,\n",
        "        avg_window=len(train_loader) // 10,\n",
        "        callbacks=[\n",
        "            LearningRateSchedulerCallback(TriangularLR(\n",
        "                optimizer, 100, ratio=3, steps_per_cycle=n_steps))\n",
        "        ],\n",
        "        use_tensorboard=False\n",
        "    )\n",
        "    bot.logger.info(bot.criterion)\n",
        "    bot.train(\n",
        "        n_steps,\n",
        "        log_interval=len(train_loader) // 10,\n",
        "        snapshot_interval=len(train_loader) // 4\n",
        "    )\n",
        "    bot.load_model(bot.best_performers[0][1])\n",
        "    torch.save(bot.model.state_dict(), str(\n",
        "        CACHE_DIR / f\"stage1_{args.fold}.pth\"))\n",
        "    bot.remove_checkpoints(keep=0)\n",
        "\n",
        "\n",
        "def train_stage_two(args, model, train_loader, valid_loader, criterion):\n",
        "    n_steps = len(train_loader) * args.epochs\n",
        "    optimizer = WeightDecayOptimizerWrapper(\n",
        "        setup_differential_learning_rates(\n",
        "            partial(\n",
        "                torch.optim.Adam, weight_decay=0\n",
        "                # AdaBound, weight_decay=0, gamma=1/5000, betas=(.8, .999)\n",
        "                # torch.optim.SGD, momentum=0.9\n",
        "            ), model, [1e-5, 8e-5, 5e-4], [1., 1., 1.]\n",
        "        ), weight_decay=5e-2, change_with_lr=True)\n",
        "    freeze_layers(model, [False, False, False])\n",
        "    bot = ImageClassificationBot(\n",
        "        model=model, train_loader=train_loader,\n",
        "        val_loader=valid_loader, clip_grad=10.,\n",
        "        optimizer=optimizer, echo=True,\n",
        "        criterion=criterion,\n",
        "        avg_window=len(train_loader) // 15,\n",
        "        callbacks=[\n",
        "            LearningRateSchedulerCallback(\n",
        "                TriangularLR(\n",
        "                    optimizer, 100, ratio=4, steps_per_cycle=n_steps\n",
        "                )\n",
        "                # GradualWarmupScheduler(\n",
        "                # optimizer, 100, len(train_loader),\n",
        "                # after_scheduler=CosineAnnealingLR(\n",
        "                #     optimizer, n_steps - len(train_loader)\n",
        "                # )\n",
        "            ),\n",
        "            MixUpCallback(alpha=0.2)\n",
        "        ],\n",
        "        use_tensorboard=True\n",
        "    )\n",
        "    bot.logger.info(bot.criterion)\n",
        "    bot.model.load_state_dict(torch.load(\n",
        "        CACHE_DIR / f\"stage1_{args.fold}.pth\"))\n",
        "\n",
        "    # def snapshot_or_not(step):\n",
        "    #     if step < 4000:\n",
        "    #         if step % 2000 == 0:\n",
        "    #             return True\n",
        "    #     elif (step - 4000) % 1000 == 0:\n",
        "    #         return True\n",
        "    #     return False\n",
        "\n",
        "    bot.train(\n",
        "        n_steps,\n",
        "        log_interval=len(train_loader) // 20,\n",
        "        snapshot_interval=len(train_loader) // 2,\n",
        "        # snapshot_interval=snapshot_or_not,\n",
        "        early_stopping_cnt=args.early_stop,\n",
        "        min_improv=1e-4,\n",
        "        keep_n_snapshots=1\n",
        "    )\n",
        "    bot.load_model(bot.best_performers[0][1])\n",
        "    bot.remove_checkpoints(keep=0)\n",
        "\n",
        "    # Final model\n",
        "    torch.save(bot.model, MODEL_DIR / f\"final_{args.fold}.pth\")\n",
        "    # Failover (args + state dict)\n",
        "    torch.save(\n",
        "        [args.arch, bot.model.state_dict()],\n",
        "        MODEL_DIR / f\"failover_{args.arch}_{args.fold}.pth\"\n",
        "    )\n",
        "\n",
        "\n",
        "def find_best_fbeta_threshold(truth, probs, beta=2, step=0.05):\n",
        "    best, best_thres = 0, -1\n",
        "    argsorted = probs.argsort(axis=1)\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore', category=UndefinedMetricWarning)\n",
        "        for thres in np.arange(step, .5, step):\n",
        "            current = fbeta_score(\n",
        "                truth,\n",
        "                binarize_prediction(\n",
        "                    probs, thres, argsorted\n",
        "                ).astype(\"int8\"),\n",
        "                beta=beta, average=\"samples\")\n",
        "            if current > best:\n",
        "                best = current\n",
        "                best_thres = thres\n",
        "    return best, best_thres\n",
        "\n",
        "\n",
        "def print_eval(truth, preds):\n",
        "    best_score, threshold = find_best_fbeta_threshold(\n",
        "        truth, preds, beta=2, step=0.01\n",
        "    )\n",
        "    print(f\"f2: {best_score:.4f} @ threshold {threshold:.2f}\")\n",
        "    print(f\"loss: {log_loss(truth, preds) / preds.shape[1]:.8f}\")\n",
        "\n",
        "\n",
        "def eval_model(args, valid_loaders: List[DataLoader]):\n",
        "    model_dir = MODEL_DIR / args.model\n",
        "    model = torch.load(str(model_dir / f\"final_{args.fold}.pth\"))\n",
        "    model = model.cuda()\n",
        "    bot = ImageClassificationBot(\n",
        "        model=model, train_loader=None,\n",
        "        val_loader=None, optimizer=None,\n",
        "        echo=True, criterion=None,\n",
        "        avg_window=100\n",
        "    )\n",
        "    tmp = []\n",
        "    for valid_loader in valid_loaders:\n",
        "        preds, truth = bot.predict(valid_loader, return_y=True)\n",
        "        preds = torch.sigmoid(preds)\n",
        "        tmp.append(preds.numpy())\n",
        "    # print(np.mean(tmp, axis=0, keepdims=False).shape, preds.numpy().shape)\n",
        "    final_preds = np.mean(tmp, axis=0, keepdims=False)\n",
        "    print_eval(\n",
        "        truth.numpy(),\n",
        "        final_preds\n",
        "    )\n",
        "    if args.min_samples > 0:\n",
        "        final_preds = mask_predictions(args, final_preds)\n",
        "        print_eval(\n",
        "            truth.numpy(),\n",
        "            final_preds\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_model(args, df: pd.DataFrame, loaders: List[DataLoader], name: str):\n",
        "    model_dir = MODEL_DIR / args.model\n",
        "    model = torch.load(str(model_dir / f\"final_{args.fold}.pth\"))\n",
        "    model = model.cuda()\n",
        "    bot = ImageClassificationBot(\n",
        "        model=model, train_loader=None,\n",
        "        val_loader=None, optimizer=None,\n",
        "        echo=True, criterion=None,\n",
        "        avg_window=100\n",
        "    )\n",
        "    tmp = []\n",
        "    model_dir = MODEL_DIR / args.model\n",
        "    for loader in loaders:\n",
        "        preds = bot.predict(loader, return_y=False)\n",
        "        preds = torch.sigmoid(preds)\n",
        "        tmp.append(preds.numpy())\n",
        "    final_preds = np.mean(tmp, axis=0, keepdims=False)\n",
        "    # print(np.isnan(final_preds).sum())\n",
        "    df_preds = pd.DataFrame(final_preds, index=df[\"id\"].values)\n",
        "    df_preds.to_pickle(CACHE_DIR / f\"preds_{name}_{args.fold}.pkl\")\n",
        "\n",
        "\n",
        "def mask_predictions(args, preds):\n",
        "    folds = pd.read_pickle('/content/folds_real.pkl')\n",
        "    mask = folds.iloc[:, 1:-1].sum(axis=0).values < args.min_samples\n",
        "    print(mask.shape, preds.shape)\n",
        "    print(f\"Masking {sum(mask)} labels...\")\n",
        "    preds[:, mask] = 0\n",
        "    return preds\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    arg = parser.add_argument\n",
        "    arg('mode', choices=['train', 'validate',\n",
        "                         'predict_valid', 'predict_test'])\n",
        "    arg('--batch-size', type=int, default=16)\n",
        "    arg('--step', type=int, default=1)\n",
        "    arg('--tta', type=bool, default=False)\n",
        "    arg('--epochs', type=int, default=10)\n",
        "    arg('--arch', type=str, default='densenet121')\n",
        "    arg('--min-samples', type=int, default=0)\n",
        "    arg('--debug', action='store_true')\n",
        "    arg('--limit', type=int)\n",
        "    arg('--alpha', type=float, default=.5)\n",
        "    arg('--gamma', type=float, default=.25)\n",
        "    arg('--fold', type=int, default=0)\n",
        "    arg('--im_size', type=int, default=512 )\n",
        "    arg('--model', type=str, default=\".\")\n",
        "    arg('--early-stop', type=int, default=5)\n",
        "    choice = ['train']\n",
        "    args = parser.parse_args(choice)\n",
        "\n",
        "    if args.mode in (\"train\", \"validate\", \"predict_valid\"):\n",
        "        folds = pd.read_pickle('/content/folds_real.pkl')\n",
        "        N_CLASSES = len(list(folds.columns)) - 2\n",
        "        train_root = DATA_ROOT\n",
        "        train_fold = folds[folds['fold'] != args.fold].iloc[:,:-1]\n",
        "        valid_fold = folds[folds['fold'] == args.fold].iloc[:,:-1]\n",
        "        if args.limit:\n",
        "            train_fold = train_fold[:args.limit]\n",
        "            valid_fold = valid_fold[:args.limit]\n",
        "\n",
        "    use_cuda = cuda.is_available()\n",
        "    train_transform = get_transform_train(args.im_size)\n",
        "    test_transform = get_transform_test(args.tta,args.im_size)\n",
        "    if args.mode == 'train':\n",
        "        if args.arch == 'seresnext50':\n",
        "            model = get_seresnet_model(\n",
        "                arch=\"se_resnext50_32x4d\",\n",
        "                n_classes=N_CLASSES, pretrained=True if args.mode == 'train' else False)\n",
        "        elif args.arch == 'seresnext101':\n",
        "            model = get_seresnet_model(\n",
        "                arch=\"se_resnext101_32x4d\",\n",
        "                n_classes=N_CLASSES, pretrained=True if args.mode == 'train' else False)\n",
        "        elif args.arch == 'seresnext50-partial':\n",
        "            #train_transform = get_train_transform(cv2.BORDER_CONSTANT)\n",
        "            model = get_seresnet_partial_model(\n",
        "                arch=\"se_resnext50_32x4d\",\n",
        "                n_classes=N_CLASSES, pretrained=True if args.mode == 'train' else False)\n",
        "        elif args.arch.startswith(\"densenet\"):\n",
        "            model = get_densenet_model(arch=args.arch)\n",
        "        # elif args.arch.startswith(\"efficientnet\"):\n",
        "        #     model = get_efficientnet(arch=args.arch)\n",
        "        else:\n",
        "            raise ValueError(\"No such model\")\n",
        "        if use_cuda:\n",
        "            model = model.cuda()\n",
        "        # criterion = nn.BCEWithLogitsLoss()\n",
        "        criterion = FocalLoss(gamma=args.gamma, alpha=args.alpha)\n",
        "        (CACHE_DIR / 'params.json').write_text(\n",
        "            json.dumps(vars(args), indent=4, sort_keys=True))\n",
        "\n",
        "        train_loader = make_loader(\n",
        "            args, TrainDataset, train_root, train_fold, train_transform, drop_last=True, shuffle=True)\n",
        "        valid_loader = make_loader(\n",
        "            args, TrainDataset, train_root, valid_fold, test_transform, shuffle=False)\n",
        "\n",
        "        print(f'{len(train_loader.dataset):,} items in train, '\n",
        "              f'{len(valid_loader.dataset):,} in valid')\n",
        "\n",
        "        # Stage 1\n",
        "        train_stage_one(args, model, train_loader, valid_loader, criterion)\n",
        "\n",
        "        # Stage 2\n",
        "        train_stage_two(args, model, train_loader, valid_loader, criterion)\n",
        "\n",
        "    elif args.mode == 'validate':\n",
        "        valid_loaders = [\n",
        "            make_loader(\n",
        "                args, TrainDataset, train_root,\n",
        "                valid_fold, get_test_transform(), shuffle=False, drop_last=False),\n",
        "            make_loader(\n",
        "                args, TrainDataset, train_root,\n",
        "                valid_fold, get_test_transform(flip=True), shuffle=False, drop_last=False)\n",
        "        ]\n",
        "        eval_model(args, valid_loaders)\n",
        "    elif args.mode.startswith('predict'):\n",
        "        if args.mode == 'predict_valid':\n",
        "            loaders = [\n",
        "                make_loader(\n",
        "                    args, TestDataset, train_root,\n",
        "                    valid_fold, get_test_transform(), shuffle=False, drop_last=False),\n",
        "                make_loader(\n",
        "                    args, TestDataset, train_root,\n",
        "                    valid_fold, get_test_transform(flip=True), shuffle=False, drop_last=False)\n",
        "            ]\n",
        "            predict_model(args, valid_fold, loaders, \"valid\")\n",
        "        elif args.mode == 'predict_test':\n",
        "            test_root = DATA_ROOT\n",
        "            df_test = pd.read_csv(DATA_ROOT / 'sample_submission.csv')\n",
        "            if args.limit:\n",
        "                df_test = df_test[:args.limit]\n",
        "            print(df_test.shape)\n",
        "            loaders = [\n",
        "                make_loader(\n",
        "                    args, TestDataset, test_root, df_test,\n",
        "                    get_test_transform(), shuffle=False, drop_last=False),\n",
        "                make_loader(\n",
        "                    args, TestDataset, test_root, df_test,\n",
        "                    get_test_transform(flip=True), shuffle=False, drop_last=False)\n",
        "            ]\n",
        "            predict_model(args, df_test, loaders, \"test\")\n",
        "\n",
        "\n",
        "def binarize_prediction(probabilities, threshold: float, argsorted=None,\n",
        "                        min_labels=1, max_labels=10):\n",
        "    \"\"\" Return matrix of 0/1 predictions, same shape as probabilities.\n",
        "    \"\"\"\n",
        "    assert probabilities.shape[1] == N_CLASSES\n",
        "    if argsorted is None:\n",
        "        argsorted = probabilities.argsort(axis=1)\n",
        "    max_mask = _make_mask(argsorted, max_labels)\n",
        "    min_mask = _make_mask(argsorted, min_labels)\n",
        "    prob_mask = probabilities > threshold\n",
        "    return (max_mask & prob_mask) | min_mask\n",
        "\n",
        "\n",
        "def _make_mask(argsorted, top_n: int):\n",
        "    mask = np.zeros_like(argsorted, dtype=np.uint8)\n",
        "    col_indices = argsorted[:, -top_n:].reshape(-1)\n",
        "    row_indices = [i // top_n for i in range(len(col_indices))]\n",
        "    mask[row_indices, col_indices] = 1\n",
        "    return mask\n",
        "def children(m):\n",
        "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
        "\n",
        "\n",
        "def set_trainable_attr(m, b):\n",
        "    m.trainable = b\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = b\n",
        "\n",
        "\n",
        "def apply_leaf(m, f):\n",
        "    c = children(m)\n",
        "    if isinstance(m, nn.Module):\n",
        "        f(m)\n",
        "    if len(c) > 0:\n",
        "        for l in c:\n",
        "            apply_leaf(l, f)\n",
        "\n",
        "\n",
        "def set_trainable(l, b):\n",
        "    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n",
        "\n",
        "\n",
        "def freeze_layers(layer_groups: List, freeze_flags: List[bool]):\n",
        "    assert len(freeze_flags) == len(layer_groups)\n",
        "    for layer, flag in zip(layer_groups, freeze_flags):\n",
        "        set_trainable(layer, not flag)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "[[12/29/2019 07:58:47 PM]] SEED: 9\n",
            "[[12/29/2019 07:58:47 PM]] # of parameters: 6,962,056\n",
            "[[12/29/2019 07:58:47 PM]] # of trainable parameters: 8,200\n",
            "[[12/29/2019 07:58:47 PM]] <Focal Loss alpha=0.5 gamma=0.25>\n",
            "[[12/29/2019 07:58:47 PM]] Optimizer Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.002\n",
            "    lr: 2e-05\n",
            "    weight_decay: 0\n",
            ")\n",
            "[[12/29/2019 07:58:47 PM]] Batches per epoch: 104\n",
            "[[12/29/2019 07:58:47 PM]] ====================Epoch 1====================\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "initing linear\n",
            "1429632 | 5524224 | 8200\n",
            "1,666 items in train, 417 in valid\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[[12/29/2019 07:58:50 PM]] Step 10: train 0.581800 lr: 1.238e-03\n",
            "[[12/29/2019 07:58:53 PM]] Step 20: train 0.128985 lr: 1.746e-03\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.95it/s]\n",
            "[[12/29/2019 07:59:01 PM]] Criterion loss: 0.146828\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "[[12/29/2019 07:59:01 PM]] fbeta: 0.9084 @ 0.05\n",
            "[[12/29/2019 07:59:01 PM]] Snapshot metric -0.90835903\n",
            "[[12/29/2019 07:59:01 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.90835903_26.pth...\n",
            "[[12/29/2019 07:59:01 PM]] New low\n",
            "\n",
            "[[12/29/2019 07:59:03 PM]] Step 30: train 0.156107 lr: 1.238e-03\n",
            "[[12/29/2019 07:59:06 PM]] Step 40: train 0.135620 lr: 7.308e-04\n",
            "[[12/29/2019 07:59:09 PM]] Step 50: train 0.152399 lr: 2.231e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n",
            "[[12/29/2019 07:59:16 PM]] Criterion loss: 0.138011\n",
            "[[12/29/2019 07:59:16 PM]] fbeta: 0.9086 @ 0.35\n",
            "[[12/29/2019 07:59:16 PM]] Snapshot metric -0.90862548\n",
            "[[12/29/2019 07:59:16 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.90862548_52.pth...\n",
            "[[12/29/2019 07:59:16 PM]] New low\n",
            "\n",
            "[[12/29/2019 07:59:16 PM]] SEED: 9\n",
            "[[12/29/2019 07:59:16 PM]] # of parameters: 6,962,056\n",
            "[[12/29/2019 07:59:16 PM]] # of trainable parameters: 6,962,056\n",
            "[[12/29/2019 07:59:16 PM]] <Focal Loss alpha=0.5 gamma=0.25>\n",
            "[[12/29/2019 07:59:16 PM]] Optimizer Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    final_lr: 1.0\n",
            "    initial_lr: 1e-05\n",
            "    lr: 1.0000000000000001e-07\n",
            "    weight_decay: 0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    final_lr: 1.0\n",
            "    initial_lr: 8e-05\n",
            "    lr: 8.000000000000001e-07\n",
            "    weight_decay: 0\n",
            "\n",
            "Parameter Group 2\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    final_lr: 1.0\n",
            "    initial_lr: 0.0005\n",
            "    lr: 5e-06\n",
            "    weight_decay: 0\n",
            ")\n",
            "[[12/29/2019 07:59:16 PM]] Batches per epoch: 104\n",
            "[[12/29/2019 07:59:16 PM]] ====================Epoch 1====================\n",
            "[[12/29/2019 07:59:20 PM]] Step 5: train 0.190407 lr: 1.214e-05\n",
            "[[12/29/2019 07:59:23 PM]] Step 10: train 0.169861 lr: 2.404e-05\n",
            "[[12/29/2019 07:59:26 PM]] Step 15: train 0.137408 lr: 3.594e-05\n",
            "[[12/29/2019 07:59:29 PM]] Step 20: train 0.153665 lr: 4.784e-05\n",
            "[[12/29/2019 07:59:33 PM]] Step 25: train 0.152772 lr: 5.974e-05\n",
            "[[12/29/2019 07:59:36 PM]] Step 30: train 0.153939 lr: 7.163e-05\n",
            "[[12/29/2019 07:59:39 PM]] Step 35: train 0.125140 lr: 8.353e-05\n",
            "[[12/29/2019 07:59:42 PM]] Step 40: train 0.101021 lr: 9.543e-05\n",
            "[[12/29/2019 07:59:46 PM]] Step 45: train 0.094911 lr: 1.073e-04\n",
            "[[12/29/2019 07:59:49 PM]] Step 50: train 0.147383 lr: 1.192e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.91it/s]\n",
            "[[12/29/2019 07:59:57 PM]] Criterion loss: 0.110722\n",
            "[[12/29/2019 07:59:57 PM]] fbeta: 0.9090 @ 0.30\n",
            "[[12/29/2019 07:59:57 PM]] Snapshot metric -0.90902516\n",
            "[[12/29/2019 07:59:57 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.90902516_52.pth...\n",
            "[[12/29/2019 07:59:57 PM]] New low\n",
            "\n",
            "[[12/29/2019 07:59:59 PM]] Step 55: train 0.126491 lr: 1.311e-04\n",
            "[[12/29/2019 08:00:02 PM]] Step 60: train 0.134849 lr: 1.430e-04\n",
            "[[12/29/2019 08:00:05 PM]] Step 65: train 0.133596 lr: 1.549e-04\n",
            "[[12/29/2019 08:00:09 PM]] Step 70: train 0.139629 lr: 1.668e-04\n",
            "[[12/29/2019 08:00:12 PM]] Step 75: train 0.095627 lr: 1.787e-04\n",
            "[[12/29/2019 08:00:15 PM]] Step 80: train 0.092551 lr: 1.906e-04\n",
            "[[12/29/2019 08:00:18 PM]] Step 85: train 0.097000 lr: 2.025e-04\n",
            "[[12/29/2019 08:00:22 PM]] Step 90: train 0.103528 lr: 2.144e-04\n",
            "[[12/29/2019 08:00:25 PM]] Step 95: train 0.122259 lr: 2.263e-04\n",
            "[[12/29/2019 08:00:28 PM]] Step 100: train 0.118046 lr: 2.382e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.91it/s]\n",
            "[[12/29/2019 08:00:38 PM]] Criterion loss: 0.095194\n",
            "[[12/29/2019 08:00:38 PM]] fbeta: 0.9143 @ 0.15\n",
            "[[12/29/2019 08:00:38 PM]] Snapshot metric -0.91430113\n",
            "[[12/29/2019 08:00:38 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.91430113_104.pth...\n",
            "[[12/29/2019 08:00:38 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:00:38 PM]] ====================Epoch 2====================\n",
            "[[12/29/2019 08:00:38 PM]] Step 105: train 0.115033 lr: 2.501e-04\n",
            "[[12/29/2019 08:00:42 PM]] Step 110: train 0.154848 lr: 2.620e-04\n",
            "[[12/29/2019 08:00:45 PM]] Step 115: train 0.096427 lr: 2.739e-04\n",
            "[[12/29/2019 08:00:48 PM]] Step 120: train 0.079977 lr: 2.858e-04\n",
            "[[12/29/2019 08:00:51 PM]] Step 125: train 0.083305 lr: 2.977e-04\n",
            "[[12/29/2019 08:00:55 PM]] Step 130: train 0.099529 lr: 3.096e-04\n",
            "[[12/29/2019 08:00:58 PM]] Step 135: train 0.109474 lr: 3.215e-04\n",
            "[[12/29/2019 08:01:01 PM]] Step 140: train 0.111384 lr: 3.334e-04\n",
            "[[12/29/2019 08:01:04 PM]] Step 145: train 0.096530 lr: 3.453e-04\n",
            "[[12/29/2019 08:01:08 PM]] Step 150: train 0.093874 lr: 3.572e-04\n",
            "[[12/29/2019 08:01:11 PM]] Step 155: train 0.110586 lr: 3.691e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n",
            "[[12/29/2019 08:01:18 PM]] Criterion loss: 0.089376\n",
            "[[12/29/2019 08:01:18 PM]] fbeta: 0.9153 @ 0.15\n",
            "[[12/29/2019 08:01:18 PM]] Snapshot metric -0.91525621\n",
            "[[12/29/2019 08:01:18 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.91525621_156.pth...\n",
            "[[12/29/2019 08:01:19 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:01:21 PM]] Step 160: train 0.095853 lr: 3.810e-04\n",
            "[[12/29/2019 08:01:24 PM]] Step 165: train 0.094342 lr: 3.929e-04\n",
            "[[12/29/2019 08:01:28 PM]] Step 170: train 0.072281 lr: 4.048e-04\n",
            "[[12/29/2019 08:01:31 PM]] Step 175: train 0.081575 lr: 4.167e-04\n",
            "[[12/29/2019 08:01:34 PM]] Step 180: train 0.091832 lr: 4.286e-04\n",
            "[[12/29/2019 08:01:37 PM]] Step 185: train 0.081194 lr: 4.405e-04\n",
            "[[12/29/2019 08:01:40 PM]] Step 190: train 0.125677 lr: 4.524e-04\n",
            "[[12/29/2019 08:01:44 PM]] Step 195: train 0.100074 lr: 4.643e-04\n",
            "[[12/29/2019 08:01:47 PM]] Step 200: train 0.106824 lr: 4.762e-04\n",
            "[[12/29/2019 08:01:50 PM]] Step 205: train 0.088994 lr: 4.881e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.95it/s]\n",
            "[[12/29/2019 08:01:59 PM]] Criterion loss: 0.081616\n",
            "[[12/29/2019 08:01:59 PM]] fbeta: 0.9204 @ 0.30\n",
            "[[12/29/2019 08:01:59 PM]] Snapshot metric -0.92042729\n",
            "[[12/29/2019 08:01:59 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.92042729_208.pth...\n",
            "[[12/29/2019 08:01:59 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:01:59 PM]] ====================Epoch 3====================\n",
            "[[12/29/2019 08:02:00 PM]] Step 210: train 0.101335 lr: 5.000e-04\n",
            "[[12/29/2019 08:02:04 PM]] Step 215: train 0.087390 lr: 4.970e-04\n",
            "[[12/29/2019 08:02:07 PM]] Step 220: train 0.094299 lr: 4.941e-04\n",
            "[[12/29/2019 08:02:10 PM]] Step 225: train 0.101373 lr: 4.911e-04\n",
            "[[12/29/2019 08:02:13 PM]] Step 230: train 0.093585 lr: 4.881e-04\n",
            "[[12/29/2019 08:02:17 PM]] Step 235: train 0.079556 lr: 4.851e-04\n",
            "[[12/29/2019 08:02:20 PM]] Step 240: train 0.080647 lr: 4.822e-04\n",
            "[[12/29/2019 08:02:23 PM]] Step 245: train 0.133036 lr: 4.792e-04\n",
            "[[12/29/2019 08:02:26 PM]] Step 250: train 0.108676 lr: 4.762e-04\n",
            "[[12/29/2019 08:02:29 PM]] Step 255: train 0.082992 lr: 4.732e-04\n",
            "[[12/29/2019 08:02:33 PM]] Step 260: train 0.069548 lr: 4.703e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.96it/s]\n",
            "[[12/29/2019 08:02:40 PM]] Criterion loss: 0.079596\n",
            "[[12/29/2019 08:02:40 PM]] fbeta: 0.9277 @ 0.30\n",
            "[[12/29/2019 08:02:40 PM]] Snapshot metric -0.92766999\n",
            "[[12/29/2019 08:02:40 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.92766999_260.pth...\n",
            "[[12/29/2019 08:02:40 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:02:43 PM]] Step 265: train 0.087665 lr: 4.673e-04\n",
            "[[12/29/2019 08:02:46 PM]] Step 270: train 0.091869 lr: 4.643e-04\n",
            "[[12/29/2019 08:02:49 PM]] Step 275: train 0.108977 lr: 4.613e-04\n",
            "[[12/29/2019 08:02:53 PM]] Step 280: train 0.071470 lr: 4.584e-04\n",
            "[[12/29/2019 08:02:56 PM]] Step 285: train 0.079764 lr: 4.554e-04\n",
            "[[12/29/2019 08:02:59 PM]] Step 290: train 0.092312 lr: 4.524e-04\n",
            "[[12/29/2019 08:03:02 PM]] Step 295: train 0.081554 lr: 4.494e-04\n",
            "[[12/29/2019 08:03:06 PM]] Step 300: train 0.102271 lr: 4.465e-04\n",
            "[[12/29/2019 08:03:09 PM]] Step 305: train 0.062458 lr: 4.435e-04\n",
            "[[12/29/2019 08:03:12 PM]] Step 310: train 0.079000 lr: 4.405e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.94it/s]\n",
            "[[12/29/2019 08:03:20 PM]] Criterion loss: 0.078488\n",
            "[[12/29/2019 08:03:20 PM]] fbeta: 0.9248 @ 0.25\n",
            "[[12/29/2019 08:03:20 PM]] Snapshot metric -0.92475111\n",
            "[[12/29/2019 08:03:20 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.92475111_312.pth...\n",
            "[[12/29/2019 08:03:20 PM]] ====================Epoch 4====================\n",
            "[[12/29/2019 08:03:22 PM]] Step 315: train 0.079153 lr: 4.375e-04\n",
            "[[12/29/2019 08:03:25 PM]] Step 320: train 0.093969 lr: 4.346e-04\n",
            "[[12/29/2019 08:03:29 PM]] Step 325: train 0.089467 lr: 4.316e-04\n",
            "[[12/29/2019 08:03:32 PM]] Step 330: train 0.113981 lr: 4.286e-04\n",
            "[[12/29/2019 08:03:35 PM]] Step 335: train 0.086518 lr: 4.256e-04\n",
            "[[12/29/2019 08:03:38 PM]] Step 340: train 0.107985 lr: 4.227e-04\n",
            "[[12/29/2019 08:03:42 PM]] Step 345: train 0.066731 lr: 4.197e-04\n",
            "[[12/29/2019 08:03:45 PM]] Step 350: train 0.071956 lr: 4.167e-04\n",
            "[[12/29/2019 08:03:48 PM]] Step 355: train 0.090935 lr: 4.137e-04\n",
            "[[12/29/2019 08:03:51 PM]] Step 360: train 0.076105 lr: 4.108e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n",
            "[[12/29/2019 08:04:01 PM]] Criterion loss: 0.071810\n",
            "[[12/29/2019 08:04:01 PM]] fbeta: 0.9306 @ 0.40\n",
            "[[12/29/2019 08:04:01 PM]] Snapshot metric -0.93063731\n",
            "[[12/29/2019 08:04:01 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93063731_364.pth...\n",
            "[[12/29/2019 08:04:01 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:04:02 PM]] Step 365: train 0.068732 lr: 4.078e-04\n",
            "[[12/29/2019 08:04:05 PM]] Step 370: train 0.081508 lr: 4.048e-04\n",
            "[[12/29/2019 08:04:08 PM]] Step 375: train 0.067932 lr: 4.018e-04\n",
            "[[12/29/2019 08:04:11 PM]] Step 380: train 0.076666 lr: 3.989e-04\n",
            "[[12/29/2019 08:04:14 PM]] Step 385: train 0.075714 lr: 3.959e-04\n",
            "[[12/29/2019 08:04:18 PM]] Step 390: train 0.065238 lr: 3.929e-04\n",
            "[[12/29/2019 08:04:21 PM]] Step 395: train 0.071922 lr: 3.899e-04\n",
            "[[12/29/2019 08:04:24 PM]] Step 400: train 0.084816 lr: 3.870e-04\n",
            "[[12/29/2019 08:04:27 PM]] Step 405: train 0.089280 lr: 3.840e-04\n",
            "[[12/29/2019 08:04:31 PM]] Step 410: train 0.084715 lr: 3.810e-04\n",
            "[[12/29/2019 08:04:34 PM]] Step 415: train 0.098867 lr: 3.780e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.96it/s]\n",
            "[[12/29/2019 08:04:41 PM]] Criterion loss: 0.072548\n",
            "[[12/29/2019 08:04:41 PM]] fbeta: 0.9291 @ 0.30\n",
            "[[12/29/2019 08:04:41 PM]] Snapshot metric -0.92905070\n",
            "[[12/29/2019 08:04:41 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.92905070_416.pth...\n",
            "[[12/29/2019 08:04:42 PM]] ====================Epoch 5====================\n",
            "[[12/29/2019 08:04:44 PM]] Step 420: train 0.104883 lr: 3.751e-04\n",
            "[[12/29/2019 08:04:47 PM]] Step 425: train 0.083157 lr: 3.721e-04\n",
            "[[12/29/2019 08:04:51 PM]] Step 430: train 0.107646 lr: 3.691e-04\n",
            "[[12/29/2019 08:04:54 PM]] Step 435: train 0.083540 lr: 3.661e-04\n",
            "[[12/29/2019 08:04:57 PM]] Step 440: train 0.092429 lr: 3.632e-04\n",
            "[[12/29/2019 08:05:00 PM]] Step 445: train 0.083639 lr: 3.602e-04\n",
            "[[12/29/2019 08:05:03 PM]] Step 450: train 0.069298 lr: 3.572e-04\n",
            "[[12/29/2019 08:05:07 PM]] Step 455: train 0.087367 lr: 3.542e-04\n",
            "[[12/29/2019 08:05:10 PM]] Step 460: train 0.100484 lr: 3.513e-04\n",
            "[[12/29/2019 08:05:13 PM]] Step 465: train 0.089953 lr: 3.483e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.94it/s]\n",
            "[[12/29/2019 08:05:22 PM]] Criterion loss: 0.065493\n",
            "[[12/29/2019 08:05:22 PM]] fbeta: 0.9388 @ 0.30\n",
            "[[12/29/2019 08:05:22 PM]] Snapshot metric -0.93877625\n",
            "[[12/29/2019 08:05:22 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93877625_468.pth...\n",
            "[[12/29/2019 08:05:22 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:05:23 PM]] Step 470: train 0.067314 lr: 3.453e-04\n",
            "[[12/29/2019 08:05:27 PM]] Step 475: train 0.076972 lr: 3.423e-04\n",
            "[[12/29/2019 08:05:30 PM]] Step 480: train 0.097927 lr: 3.394e-04\n",
            "[[12/29/2019 08:05:33 PM]] Step 485: train 0.079885 lr: 3.364e-04\n",
            "[[12/29/2019 08:05:36 PM]] Step 490: train 0.075970 lr: 3.334e-04\n",
            "[[12/29/2019 08:05:40 PM]] Step 495: train 0.077542 lr: 3.304e-04\n",
            "[[12/29/2019 08:05:43 PM]] Step 500: train 0.080065 lr: 3.275e-04\n",
            "[[12/29/2019 08:05:46 PM]] Step 505: train 0.086573 lr: 3.245e-04\n",
            "[[12/29/2019 08:05:49 PM]] Step 510: train 0.068924 lr: 3.215e-04\n",
            "[[12/29/2019 08:05:53 PM]] Step 515: train 0.078519 lr: 3.185e-04\n",
            "[[12/29/2019 08:05:56 PM]] Step 520: train 0.076822 lr: 3.156e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.96it/s]\n",
            "[[12/29/2019 08:06:03 PM]] Criterion loss: 0.065054\n",
            "[[12/29/2019 08:06:03 PM]] fbeta: 0.9361 @ 0.40\n",
            "[[12/29/2019 08:06:03 PM]] Snapshot metric -0.93610839\n",
            "[[12/29/2019 08:06:03 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93610839_520.pth...\n",
            "[[12/29/2019 08:06:03 PM]] ====================Epoch 6====================\n",
            "[[12/29/2019 08:06:06 PM]] Step 525: train 0.058522 lr: 3.126e-04\n",
            "[[12/29/2019 08:06:09 PM]] Step 530: train 0.071127 lr: 3.096e-04\n",
            "[[12/29/2019 08:06:12 PM]] Step 535: train 0.074895 lr: 3.066e-04\n",
            "[[12/29/2019 08:06:16 PM]] Step 540: train 0.082531 lr: 3.037e-04\n",
            "[[12/29/2019 08:06:19 PM]] Step 545: train 0.078172 lr: 3.007e-04\n",
            "[[12/29/2019 08:06:22 PM]] Step 550: train 0.086782 lr: 2.977e-04\n",
            "[[12/29/2019 08:06:25 PM]] Step 555: train 0.076643 lr: 2.947e-04\n",
            "[[12/29/2019 08:06:29 PM]] Step 560: train 0.078375 lr: 2.918e-04\n",
            "[[12/29/2019 08:06:32 PM]] Step 565: train 0.077389 lr: 2.888e-04\n",
            "[[12/29/2019 08:06:35 PM]] Step 570: train 0.075429 lr: 2.858e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.96it/s]\n",
            "[[12/29/2019 08:06:43 PM]] Criterion loss: 0.064821\n",
            "[[12/29/2019 08:06:43 PM]] fbeta: 0.9384 @ 0.35\n",
            "[[12/29/2019 08:06:43 PM]] Snapshot metric -0.93840439\n",
            "[[12/29/2019 08:06:43 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93840439_572.pth...\n",
            "[[12/29/2019 08:06:45 PM]] Step 575: train 0.078042 lr: 2.828e-04\n",
            "[[12/29/2019 08:06:48 PM]] Step 580: train 0.096119 lr: 2.799e-04\n",
            "[[12/29/2019 08:06:52 PM]] Step 585: train 0.070359 lr: 2.769e-04\n",
            "[[12/29/2019 08:06:55 PM]] Step 590: train 0.077402 lr: 2.739e-04\n",
            "[[12/29/2019 08:06:58 PM]] Step 595: train 0.061618 lr: 2.709e-04\n",
            "[[12/29/2019 08:07:01 PM]] Step 600: train 0.066561 lr: 2.680e-04\n",
            "[[12/29/2019 08:07:05 PM]] Step 605: train 0.100788 lr: 2.650e-04\n",
            "[[12/29/2019 08:07:08 PM]] Step 610: train 0.071601 lr: 2.620e-04\n",
            "[[12/29/2019 08:07:11 PM]] Step 615: train 0.084627 lr: 2.590e-04\n",
            "[[12/29/2019 08:07:14 PM]] Step 620: train 0.084522 lr: 2.561e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.97it/s]\n",
            "[[12/29/2019 08:07:24 PM]] Criterion loss: 0.065932\n",
            "[[12/29/2019 08:07:24 PM]] fbeta: 0.9399 @ 0.30\n",
            "[[12/29/2019 08:07:24 PM]] Snapshot metric -0.93993896\n",
            "[[12/29/2019 08:07:24 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93993896_624.pth...\n",
            "[[12/29/2019 08:07:24 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:07:24 PM]] ====================Epoch 7====================\n",
            "[[12/29/2019 08:07:24 PM]] Step 625: train 0.084948 lr: 2.531e-04\n",
            "[[12/29/2019 08:07:28 PM]] Step 630: train 0.068489 lr: 2.501e-04\n",
            "[[12/29/2019 08:07:31 PM]] Step 635: train 0.067271 lr: 2.471e-04\n",
            "[[12/29/2019 08:07:34 PM]] Step 640: train 0.061474 lr: 2.442e-04\n",
            "[[12/29/2019 08:07:37 PM]] Step 645: train 0.069511 lr: 2.412e-04\n",
            "[[12/29/2019 08:07:41 PM]] Step 650: train 0.067280 lr: 2.382e-04\n",
            "[[12/29/2019 08:07:44 PM]] Step 655: train 0.088433 lr: 2.352e-04\n",
            "[[12/29/2019 08:07:47 PM]] Step 660: train 0.088569 lr: 2.323e-04\n",
            "[[12/29/2019 08:07:50 PM]] Step 665: train 0.090177 lr: 2.293e-04\n",
            "[[12/29/2019 08:07:54 PM]] Step 670: train 0.063590 lr: 2.263e-04\n",
            "[[12/29/2019 08:07:57 PM]] Step 675: train 0.059764 lr: 2.233e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.92it/s]\n",
            "[[12/29/2019 08:08:04 PM]] Criterion loss: 0.065216\n",
            "[[12/29/2019 08:08:05 PM]] fbeta: 0.9353 @ 0.30\n",
            "[[12/29/2019 08:08:05 PM]] Snapshot metric -0.93527602\n",
            "[[12/29/2019 08:08:05 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93527602_676.pth...\n",
            "[[12/29/2019 08:08:07 PM]] Step 680: train 0.066061 lr: 2.204e-04\n",
            "[[12/29/2019 08:08:10 PM]] Step 685: train 0.055331 lr: 2.174e-04\n",
            "[[12/29/2019 08:08:14 PM]] Step 690: train 0.083625 lr: 2.144e-04\n",
            "[[12/29/2019 08:08:17 PM]] Step 695: train 0.062428 lr: 2.114e-04\n",
            "[[12/29/2019 08:08:20 PM]] Step 700: train 0.067612 lr: 2.085e-04\n",
            "[[12/29/2019 08:08:23 PM]] Step 705: train 0.063424 lr: 2.055e-04\n",
            "[[12/29/2019 08:08:27 PM]] Step 710: train 0.064183 lr: 2.025e-04\n",
            "[[12/29/2019 08:08:30 PM]] Step 715: train 0.059719 lr: 1.995e-04\n",
            "[[12/29/2019 08:08:33 PM]] Step 720: train 0.068683 lr: 1.966e-04\n",
            "[[12/29/2019 08:08:36 PM]] Step 725: train 0.086239 lr: 1.936e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n",
            "[[12/29/2019 08:08:45 PM]] Criterion loss: 0.064199\n",
            "[[12/29/2019 08:08:45 PM]] fbeta: 0.9380 @ 0.20\n",
            "[[12/29/2019 08:08:45 PM]] Snapshot metric -0.93797343\n",
            "[[12/29/2019 08:08:45 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93797343_728.pth...\n",
            "[[12/29/2019 08:08:45 PM]] ====================Epoch 8====================\n",
            "[[12/29/2019 08:08:47 PM]] Step 730: train 0.066345 lr: 1.906e-04\n",
            "[[12/29/2019 08:08:50 PM]] Step 735: train 0.066575 lr: 1.877e-04\n",
            "[[12/29/2019 08:08:53 PM]] Step 740: train 0.073042 lr: 1.847e-04\n",
            "[[12/29/2019 08:08:56 PM]] Step 745: train 0.060476 lr: 1.817e-04\n",
            "[[12/29/2019 08:09:00 PM]] Step 750: train 0.059837 lr: 1.787e-04\n",
            "[[12/29/2019 08:09:03 PM]] Step 755: train 0.068829 lr: 1.758e-04\n",
            "[[12/29/2019 08:09:06 PM]] Step 760: train 0.078357 lr: 1.728e-04\n",
            "[[12/29/2019 08:09:09 PM]] Step 765: train 0.071705 lr: 1.698e-04\n",
            "[[12/29/2019 08:09:13 PM]] Step 770: train 0.076195 lr: 1.668e-04\n",
            "[[12/29/2019 08:09:16 PM]] Step 775: train 0.056312 lr: 1.639e-04\n",
            "[[12/29/2019 08:09:19 PM]] Step 780: train 0.055784 lr: 1.609e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.92it/s]\n",
            "[[12/29/2019 08:09:26 PM]] Criterion loss: 0.060821\n",
            "[[12/29/2019 08:09:26 PM]] fbeta: 0.9403 @ 0.25\n",
            "[[12/29/2019 08:09:26 PM]] Snapshot metric -0.94027116\n",
            "[[12/29/2019 08:09:26 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.94027116_780.pth...\n",
            "[[12/29/2019 08:09:26 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:09:29 PM]] Step 785: train 0.067897 lr: 1.579e-04\n",
            "[[12/29/2019 08:09:33 PM]] Step 790: train 0.069535 lr: 1.549e-04\n",
            "[[12/29/2019 08:09:36 PM]] Step 795: train 0.067836 lr: 1.520e-04\n",
            "[[12/29/2019 08:09:39 PM]] Step 800: train 0.058681 lr: 1.490e-04\n",
            "[[12/29/2019 08:09:43 PM]] Step 805: train 0.068354 lr: 1.460e-04\n",
            "[[12/29/2019 08:09:46 PM]] Step 810: train 0.084418 lr: 1.430e-04\n",
            "[[12/29/2019 08:09:49 PM]] Step 815: train 0.071091 lr: 1.401e-04\n",
            "[[12/29/2019 08:09:52 PM]] Step 820: train 0.076339 lr: 1.371e-04\n",
            "[[12/29/2019 08:09:55 PM]] Step 825: train 0.065657 lr: 1.341e-04\n",
            "[[12/29/2019 08:09:59 PM]] Step 830: train 0.059639 lr: 1.311e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.92it/s]\n",
            "[[12/29/2019 08:10:07 PM]] Criterion loss: 0.062032\n",
            "[[12/29/2019 08:10:07 PM]] fbeta: 0.9427 @ 0.45\n",
            "[[12/29/2019 08:10:07 PM]] Snapshot metric -0.94273153\n",
            "[[12/29/2019 08:10:07 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.94273153_832.pth...\n",
            "[[12/29/2019 08:10:07 PM]] New low\n",
            "\n",
            "[[12/29/2019 08:10:07 PM]] ====================Epoch 9====================\n",
            "[[12/29/2019 08:10:09 PM]] Step 835: train 0.076184 lr: 1.282e-04\n",
            "[[12/29/2019 08:10:12 PM]] Step 840: train 0.074952 lr: 1.252e-04\n",
            "[[12/29/2019 08:10:15 PM]] Step 845: train 0.070724 lr: 1.222e-04\n",
            "[[12/29/2019 08:10:19 PM]] Step 850: train 0.077444 lr: 1.192e-04\n",
            "[[12/29/2019 08:10:22 PM]] Step 855: train 0.063299 lr: 1.163e-04\n",
            "[[12/29/2019 08:10:25 PM]] Step 860: train 0.059734 lr: 1.133e-04\n",
            "[[12/29/2019 08:10:28 PM]] Step 865: train 0.059133 lr: 1.103e-04\n",
            "[[12/29/2019 08:10:32 PM]] Step 870: train 0.073863 lr: 1.073e-04\n",
            "[[12/29/2019 08:10:35 PM]] Step 875: train 0.062511 lr: 1.044e-04\n",
            "[[12/29/2019 08:10:38 PM]] Step 880: train 0.075276 lr: 1.014e-04\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.94it/s]\n",
            "[[12/29/2019 08:10:48 PM]] Criterion loss: 0.061790\n",
            "[[12/29/2019 08:10:48 PM]] fbeta: 0.9367 @ 0.25\n",
            "[[12/29/2019 08:10:48 PM]] Snapshot metric -0.93673286\n",
            "[[12/29/2019 08:10:48 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93673286_884.pth...\n",
            "[[12/29/2019 08:10:48 PM]] Step 885: train 0.068941 lr: 9.841e-05\n",
            "[[12/29/2019 08:10:52 PM]] Step 890: train 0.068883 lr: 9.543e-05\n",
            "[[12/29/2019 08:10:55 PM]] Step 895: train 0.067504 lr: 9.246e-05\n",
            "[[12/29/2019 08:10:58 PM]] Step 900: train 0.081586 lr: 8.948e-05\n",
            "[[12/29/2019 08:11:01 PM]] Step 905: train 0.062620 lr: 8.651e-05\n",
            "[[12/29/2019 08:11:05 PM]] Step 910: train 0.063783 lr: 8.353e-05\n",
            "[[12/29/2019 08:11:08 PM]] Step 915: train 0.066659 lr: 8.056e-05\n",
            "[[12/29/2019 08:11:11 PM]] Step 920: train 0.068705 lr: 7.758e-05\n",
            "[[12/29/2019 08:11:14 PM]] Step 925: train 0.058510 lr: 7.461e-05\n",
            "[[12/29/2019 08:11:18 PM]] Step 930: train 0.069268 lr: 7.163e-05\n",
            "[[12/29/2019 08:11:21 PM]] Step 935: train 0.083369 lr: 6.866e-05\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.97it/s]\n",
            "[[12/29/2019 08:11:28 PM]] Criterion loss: 0.059590\n",
            "[[12/29/2019 08:11:28 PM]] fbeta: 0.9386 @ 0.45\n",
            "[[12/29/2019 08:11:28 PM]] Snapshot metric -0.93861028\n",
            "[[12/29/2019 08:11:28 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.93861028_936.pth...\n",
            "[[12/29/2019 08:11:28 PM]] ====================Epoch 10====================\n",
            "[[12/29/2019 08:11:31 PM]] Step 940: train 0.071517 lr: 6.569e-05\n",
            "[[12/29/2019 08:11:34 PM]] Step 945: train 0.080387 lr: 6.271e-05\n",
            "[[12/29/2019 08:11:37 PM]] Step 950: train 0.061551 lr: 5.974e-05\n",
            "[[12/29/2019 08:11:41 PM]] Step 955: train 0.071355 lr: 5.676e-05\n",
            "[[12/29/2019 08:11:44 PM]] Step 960: train 0.066045 lr: 5.379e-05\n",
            "[[12/29/2019 08:11:47 PM]] Step 965: train 0.055068 lr: 5.081e-05\n",
            "[[12/29/2019 08:11:50 PM]] Step 970: train 0.063923 lr: 4.784e-05\n",
            "[[12/29/2019 08:11:54 PM]] Step 975: train 0.062772 lr: 4.486e-05\n",
            "[[12/29/2019 08:11:57 PM]] Step 980: train 0.057226 lr: 4.189e-05\n",
            "[[12/29/2019 08:12:00 PM]] Step 985: train 0.066338 lr: 3.891e-05\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n",
            "[[12/29/2019 08:12:09 PM]] Criterion loss: 0.062065\n",
            "[[12/29/2019 08:12:09 PM]] fbeta: 0.9401 @ 0.45\n",
            "[[12/29/2019 08:12:09 PM]] Snapshot metric -0.94007578\n",
            "[[12/29/2019 08:12:09 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.94007578_988.pth...\n",
            "[[12/29/2019 08:12:10 PM]] Step 990: train 0.074582 lr: 3.594e-05\n",
            "[[12/29/2019 08:12:14 PM]] Step 995: train 0.078157 lr: 3.296e-05\n",
            "[[12/29/2019 08:12:17 PM]] Step 1000: train 0.068088 lr: 2.999e-05\n",
            "[[12/29/2019 08:12:20 PM]] Step 1005: train 0.050594 lr: 2.701e-05\n",
            "[[12/29/2019 08:12:23 PM]] Step 1010: train 0.053818 lr: 2.404e-05\n",
            "[[12/29/2019 08:12:27 PM]] Step 1015: train 0.066131 lr: 2.106e-05\n",
            "[[12/29/2019 08:12:30 PM]] Step 1020: train 0.066874 lr: 1.809e-05\n",
            "[[12/29/2019 08:12:33 PM]] Step 1025: train 0.057223 lr: 1.511e-05\n",
            "[[12/29/2019 08:12:36 PM]] Step 1030: train 0.055253 lr: 1.214e-05\n",
            "[[12/29/2019 08:12:40 PM]] Step 1035: train 0.064255 lr: 9.165e-06\n",
            "[[12/29/2019 08:12:43 PM]] Step 1040: train 0.069441 lr: 6.190e-06\n",
            "100%|██████████| 27/27 [00:06<00:00,  3.98it/s]\n",
            "[[12/29/2019 08:12:50 PM]] Criterion loss: 0.061026\n",
            "[[12/29/2019 08:12:50 PM]] fbeta: 0.9411 @ 0.50\n",
            "[[12/29/2019 08:12:50 PM]] Snapshot metric -0.94114159\n",
            "[[12/29/2019 08:12:50 PM]] Saving checkpoint /content/cache/model_cache/snapshot_bot_-0.94114159_1040.pth...\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type _DenseBlock. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type _DenseLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type _Transition. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AvgPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Flatten. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rixAMzOKcJ5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1546857-5d6d-4f54-ec34-44a1b366f70d"
      },
      "source": [
        "!zip -r '/content/models.zip' '/content/models'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/models/ (stored 0%)\n",
            "  adding: content/models/logs/ (stored 0%)\n",
            "  adding: content/models/logs/log_20191229_1918.txt (deflated 68%)\n",
            "  adding: content/models/logs/log_20191229_1936.txt (deflated 81%)\n",
            "  adding: content/models/logs/log_20191229_1916.txt (stored 0%)\n",
            "  adding: content/models/logs/summaries/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/losses/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/losses/train/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/losses/train/events.out.tfevents.1577649560.d71236af64ec (deflated 62%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/losses/val/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/losses/val/events.out.tfevents.1577649597.d71236af64ec (deflated 50%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/events.out.tfevents.1577649556.d71236af64ec (deflated 58%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/monitor_metric/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/monitor_metric/val/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1959/monitor_metric/val/events.out.tfevents.1577649597.d71236af64ec (deflated 56%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1933/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1933/events.out.tfevents.1577648001.d71236af64ec (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1918/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1918/events.out.tfevents.1577647134.d71236af64ec (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/events.out.tfevents.1577648183.d71236af64ec (deflated 58%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/losses/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/losses/train/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/losses/train/events.out.tfevents.1577648186.d71236af64ec (deflated 62%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/losses/val/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/losses/val/events.out.tfevents.1577648224.d71236af64ec (deflated 51%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/monitor_metric/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/monitor_metric/val/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1936/monitor_metric/val/events.out.tfevents.1577648224.d71236af64ec (deflated 57%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/losses/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/losses/train/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/losses/train/events.out.tfevents.1577647546.d71236af64ec (deflated 60%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/losses/val/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/losses/val/events.out.tfevents.1577647593.d71236af64ec (deflated 43%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/events.out.tfevents.1577647542.d71236af64ec (deflated 56%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/monitor_metric/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/monitor_metric/val/ (stored 0%)\n",
            "  adding: content/models/logs/summaries/bot_20191229_1925/monitor_metric/val/events.out.tfevents.1577647593.d71236af64ec (deflated 49%)\n",
            "  adding: content/models/logs/log_20191229_1912.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1935.txt (deflated 73%)\n",
            "  adding: content/models/logs/log_20191229_1958.txt (deflated 65%)\n",
            "  adding: content/models/logs/log_20191229_1910.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1854.txt (deflated 72%)\n",
            "  adding: content/models/logs/log_20191229_1731.txt (stored 0%)\n",
            "  adding: content/models/logs/log_20191229_1734.txt (deflated 80%)\n",
            "  adding: content/models/logs/log_20191229_1859.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1913.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1904.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1932.txt (deflated 65%)\n",
            "  adding: content/models/logs/log_20191229_1933.txt (deflated 68%)\n",
            "  adding: content/models/logs/log_20191229_1925.txt (deflated 79%)\n",
            "  adding: content/models/logs/log_20191229_1732.txt (deflated 51%)\n",
            "  adding: content/models/logs/log_20191229_1903.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1738.txt (deflated 51%)\n",
            "  adding: content/models/logs/log_20191229_1914.txt (deflated 65%)\n",
            "  adding: content/models/logs/log_20191229_1829.txt (deflated 52%)\n",
            "  adding: content/models/logs/log_20191229_1739.txt (deflated 51%)\n",
            "  adding: content/models/logs/log_20191229_1959.txt (deflated 81%)\n",
            "  adding: content/models/logs/log_20191229_1740.txt (deflated 51%)\n",
            "  adding: content/models/logs/log_20191229_1902.txt (deflated 54%)\n",
            "  adding: content/models/logs/log_20191229_1737.txt (deflated 51%)\n",
            "  adding: content/models/logs/log_20191229_1853.txt (deflated 52%)\n",
            "  adding: content/models/logs/log_20191229_1828.txt (deflated 52%)\n",
            "  adding: content/models/logs/log_20191229_1915.txt (deflated 65%)\n",
            "  adding: content/models/logs/log_20191229_1901.txt (deflated 51%)\n",
            "  adding: content/models/logs/log_20191229_1752.txt (deflated 52%)\n",
            "  adding: content/models/logs/log_20191229_1758.txt (deflated 52%)\n",
            "  adding: content/models/failover_densenet121_0.pth (deflated 7%)\n",
            "  adding: content/models/final_0.pth (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}